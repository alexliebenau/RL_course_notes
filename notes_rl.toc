\contentsline {section}{\numberline {1}Introduction}{1}{}%
\contentsline {section}{\numberline {2}Lecture two: Markov Decision Process (MDP)}{2}{}%
\contentsline {subsection}{\numberline {2.1}Markov Process}{2}{}%
\contentsline {subsection}{\numberline {2.2}Markov Reward process (MRP)}{2}{}%
\contentsline {subsection}{\numberline {2.3}Markov Decision Process (MDP)}{4}{}%
\contentsline {subsection}{\numberline {2.4}The Optimal Value Function}{6}{}%
\contentsline {section}{\numberline {3}Lection Three: Planning by Dynamic Programming}{8}{}%
\contentsline {subsection}{\numberline {3.1}Introduction}{8}{}%
\contentsline {subsection}{\numberline {3.2}Synchronous Dynamic programming algorithms}{8}{}%
\contentsline {subsection}{\numberline {3.3}Asynchronous dynamic programming}{10}{}%
\contentsline {section}{\numberline {4}Lecture Four: Model-Free Prediction}{12}{}%
\contentsline {subsection}{\numberline {4.1}Monte-Carlo Learning}{12}{}%
\contentsline {subsection}{\numberline {4.2}Temporal-Difference Learning}{13}{}%
\contentsline {subsection}{\numberline {4.3}TD($\lambda $)}{16}{}%
\contentsline {section}{\numberline {5}Model-Free Control}{19}{}%
\contentsline {subsection}{\numberline {5.1}On and Off-policy learning}{19}{}%
\contentsline {subsection}{\numberline {5.2}$\epsilon $-Greedy Exploration}{19}{}%
\contentsline {subsection}{\numberline {5.3}TD($\lambda $) Control}{20}{}%
\contentsline {subsection}{\numberline {5.4}Off-Policy Learning}{23}{}%
