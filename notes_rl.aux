\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Lecture two: Markov Decision Process (MDP)}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Markov Process}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Markov Reward process (MRP)}{2}{}\protected@file@percent }
\newlabel{eq:g_t}{{4}{3}}
\newlabel{eq:averaging_mrp}{{7}{3}}
\newlabel{eq:mrp_matrix}{{8}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Markov Decision Process (MDP)}{4}{}\protected@file@percent }
\newlabel{eq:qn_avg}{{16}{5}}
\newlabel{eq:v_pi_be}{{17}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}The Optimal Value Function}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Lection Three: Planning by Dynamic Programming}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Introduction}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Synchronous Dynamic programming algorithms}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Asynchronous dynamic programming}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Lecture Four: Model-Free Prediction}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Monte-Carlo Learning}{12}{}\protected@file@percent }
\newlabel{eq:incr_mean}{{31}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Temporal-Difference Learning}{13}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The difference between Monte-Carlo and Temporal-Difference Learning.}}{14}{}\protected@file@percent }
\newlabel{img:mc_td}{{1}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Unified view of Reinforcement Learning}}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}TD($\lambda $)}{16}{}\protected@file@percent }
\newlabel{eq:fw-lambda}{{34}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Model-Free Control}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}On and Off-policy learning}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}$\epsilon $-Greedy Exploration}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}TD($\lambda $) Control}{20}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sarsa($\lambda $)-Algorithm}}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Off-Policy Learning}{23}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Q-Learning Algorithm}}{25}{}\protected@file@percent }
\gdef \@abspage@last{25}
