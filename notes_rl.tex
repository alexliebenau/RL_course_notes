\documentclass[10pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{xparse}
\usepackage{graphicx}

\begin{document}
\tableofcontents
\section{Introduction}

Script of the Reinforcment Learning Course by David Silver
as uploaded on YouTube from 2015

written by Alex Liebenau \newline
created in November 2022 \newline
free to use \newline
\newpage
\section{Lecture two: Markov Decision Process (MDP)}
\subsection{Markov Process}

The current state characterises the process -> we are told the state --> environment is fully observable\newline

Almost all RL problems can be formalised as MDPs\newline
--> Optimal control primarily deals with continious MDPs\newline
--> Any partially observable problems can be converted into MDPs\newline
--> Bandits are MDPs with one state\newline

"The future is independent of the past given the present"

\begin{equation}
	\mathbb{P} \: [S_{t+1}\; | \; S_{t}] \; = \; \mathbb{P} \: [S_{t+1} \; | \; S_{1}, \ldots , S_{t}]
\end{equation}


What happens next only depends on what happend on the state before - you can throw away anything else.\newline

For a Markov state \textit{s} and successor state \textit{s'}, the \textit{state transition probability} is defined as

\begin{equation}
\mathcal{P}_{ss'} \; = \; \mathbb{P}\:[S_{t+1}\:=\:\textit{s'}\; | \; S_{t}\:=\:\textit{s}]
\end{equation}

State transition matrix $\rho$ defines transition probabilites from all states \textit{s} to all successor states \textit{s'}.

\begin{equation}
\mathcal{P} \; = \; from 
\stackrel{\mbox{$to$}}{\begin{pmatrix}
\mathcal{P}_{11} & \ldots & \mathcal{P}_{1n} \\
\vdots & & \\
\mathcal{P}_{n1} & \ldots & \mathcal{P}_{nn} \\
\end{pmatrix}}
\end{equation}

Each row of the matrix sums to 1! \newline
A Markov process is a memoryless random process, i.e. a sequence of random states, $S_{1}, \: S_{2}, \: \ldots$ with the Markov propoerty. It is defined as a tuple $\langle \mathcal{S}, \: \mathcal{P} \rangle$.
\begin{itemize}
\item S is a (finite) set of states
\item $\mathcal{P}$ is a state transition probability matrix
\end{itemize}

\subsection{Markov Reward process (MRP)}

The Markov Reward Process is defined as a tuple $\langle \mathcal{S}, \: \mathcal{P}, \: \mathcal{R}, \: \gamma \rangle$.
\begin{itemize}
\item $\mathcal{S}$ is a (finite) set of states
\item $\mathcal{P}$ is a state transition probability matrix
\item $\mathcal{R}$ is a reward function, $\mathcal{R}_{s} \: = \: \mathbb{E}[R_{t+1} \: | \: S_{t} = s]$
\item $\gamma$ is a discount factor, $\gamma \in [0,1]$
\end{itemize}

The \textit{return} $\mathcal{G}_{t}$ is the total discounted reward from time-step \textit{t}.

\begin{equation}
\mathcal{G}_{t} \; = \; R_{t+1} \; + \; \gamma R_{t+2} \; + \; \ldots \; = \;  \sum_{k=0}^{\infty} \;\gamma^{k} \: R_{t+k+1} 
\label{eq:g_t}
\end{equation}

\begin{itemize}
\item The discount $\gamma$ is the present value of future rewards - the closer $\gamma$ is to zero, the less are later rewards accounted (e.g. more 'short-sighted').
\item The value of receiving reward $\mathcal{R}$ after $k+1$ time-steps is $ \gamma^{k} R$.
\end{itemize}

\subsubsection*{Why discount?}
\begin{itemize}
\item Unless you really trust your model and believe that everything turns out as planned, you need to discount in deviations - Uncertainty about the future may not be fully represented
\item Mathematically convenient to discount rewards, avoids infinite returns
\item If the reward is financial, immediate rewards may earn more interest than delayed rewards
\item Animal/human behaviour shows preference for immediate reward
\item It is sometimes possible to use \textit{undiscounted} Markov reward process (i.e. $\gamma = 1$), e.g. if all sequences terminate.
\end{itemize}

\subsubsection*{The value function}

The value function \textit{v(s)} gives the long-term value of state s. It defines the expected return in a MRP starting from state \textit{s}:

\begin{equation}
v(s) \; = \; \mathbb{E}\;[G_{t} \; | \; S_{t} \; = \; s]
\end{equation}

The value function can be decomposed into two parts:

\begin{itemize}
\item immediate reward $R+1$
\item discounted value of successor state $\gamma v(S_{t+1})$
\end{itemize}

This resolves into the \textbf{Bellman equation for MRPs}:
\begin{equation}
v(s) \; = \; \mathbb{E}\;[G_{t} \; | \; S_{t} \; = \; s] \; = \; \mathbb{E}\;[R_{t+1} \; + \; \gamma v(S_{t+1}) \; | \; S_{t} \; = \; s]
\end{equation}

By averaging all possible outcomes we get
\begin{equation}
v(s) \; = \; \mathcal{R}_{s} \; + \; \gamma \: \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}\:v(s')
 \label{eq:averaging_mrp}
\end{equation}

The Bellman  equation can be concisely using matrices, where $v$ is a column vector with one entry per state

\begin{equation}
v \; = \; \mathcal{R} \; + \; \gamma \: \mathcal{P} \: v \quad \rightarrow \quad
\begin{pmatrix}
v(1) \\ \vdots \\ v(n) \end{pmatrix} \; = \; \begin{pmatrix}
\mathcal{R}_{1} \\ \vdots \\ \mathcal{R}_{n} \end{pmatrix} \; + \; \gamma \; \begin{pmatrix}
\mathcal{P}_{11} & \ldots & \mathcal{P}_{1n} \\
\vdots & & \\
\mathcal{P}_{n1} & \ldots & \mathcal{P}_{nn} \\ \end{pmatrix} \; \begin{pmatrix}
v(1) \\ \vdots \\ v(n) \end{pmatrix} \label{eq:mrp_matrix}
\end{equation} 

The Bellman equation is linear. It can be solved directly by $v = (I - \gamma \mathcal{P})^{-1} \mathcal{R}$.

\begin{itemize}
\item The Computational complexity is O($n^{3}$) for \textit{n} states.
\item Direct solution only possible for small MRPs
\item Iterative methods for large MRPs, e.g. Dynamic programming, Monte-Carlo evaluation, Temporal-Difference learning
\end{itemize}

\subsection{Markov Decision Process (MDP)}
A MDP is a MRP with decisions. It is an \textit{environment} in which all states are Markov. \newline
A MDP is a tuple $\langle \mathcal{S, A, P, R,} \gamma \rangle$.

\begin{itemize}
\item $\mathcal{S}$ is a (finite) set of states
\item $\mathcal{A}$ is a (finite) set of actions
\item $\mathcal{P}$ is a state transition probability matrix \newline
$\mathcal{P}_{ss'}^{a} \; = \; \mathbb{P}\:[S_{t+1}\:=\:\textit{s'}\; | \; S_{t}\:=\:\textit{s}, \; A_{t}\:=\:\textit{a}]$
\item $\mathcal{R}$ is a reward function, $\mathcal{R}_{s}^{a}  \: = \: \mathbb{E}[R_{t+1} \: | \: S_{t} = s, \; A_{t}\:=\:\textit{a}]$
\item $\gamma$ is a discount factor, $\gamma \in [0,1]$
\end{itemize}

A policy $\pi$ is a distribution over actions given states. It fully defines the behaviour of an agent.
\begin{equation}
\pi(a|s) \; = \; \mathbb{P}\:[\:\: S_{t} = s, \; A_{t}\:=\:\textit{a}\:]
\end{equation}

In an MDP, the policies depend on the current state (not the history). Policies are stationary: $A_{t} = \pi ( \dot | S_{t} ), \forall t > 0$ \newline

Given an MDP $\mathcal{M} = \langle \mathcal{S, A, P, R,} \gamma \rangle$ and a policy $\pi$:
\begin{itemize}
\item The state sequence $S_{1}, S_{2}, \ldots$ is a Markov process $\langle \mathcal{S, P^{\pi}} \rangle$
\item The state and reward sequence $S_{1}, ,R_{2}, S_{2}, \ldots$ is a Markov reward process $\langle \mathcal{S}, \: \mathcal{P}, \: \mathcal{R}, \: \gamma \rangle$ where
\begin{equation}
\mathcal{P}_{s,s'}^{\pi} \; = \; \sum_{a \in \mathcal{A}} \; \pi (a|s)\: \mathcal{P}_{s,s'}^{a} \qquad
\mathcal{R}_{s}^{\pi} \; = \; \sum_{a \in \mathcal{A}} \; \pi (a|s)\: \mathcal{R}_{s}^{a} 
\end{equation}
\end{itemize}

The \textit{state-value function} $v_{\pi}(s)$ of an MDP is the expected return starting from state \textit{s}, and then following policy $\pi$:
\begin{equation}
v_{\pi}(s) \; = \; \mathbb{E}_{\pi} [ G_{t} \: | \: S_{t} \: = \: s] 
\end{equation}

The \textit{action-value function} $q_{\pi}(s, a)$ of an MDP is the expected return starting from state \textit{s}, taking action \textit{a}, and then following policy $\pi$:
\begin{equation}
q_{\pi}(s,a) \; = \; \mathbb{E}_{\pi} [ G_{t} \: | \: S_{t} \: = \: s, \: A_{t} \: = \: a] 
\end{equation}

The state-value and action-value functions can again be decomposed into Bellman equations consisting of immediate reward plus discounted value of successor state:

\begin{equation}
v_{\pi} \; = \; \mathbb{E}_{\pi} \:[ R_{t+1} \: + \: \gamma v_{\pi} (S_{t+1}) \: \ | \: S_{t} \: = \: s]
\end{equation}
\begin{equation}
q_{\pi} \; = \; \mathbb{E}_{\pi} \:[ R_{t+1} \: + \: \gamma v_{\pi} (S_{t+1}, \: A_{t+1}) \: \ | \: S_{t} \: = \: s, \: A_{t} \: = \: a]
\end{equation}

Basically, the state-value averages over the different actions that can be taken:

\begin{equation}
v_{\pi}(s) \; = \; \sum_{a \in \mathcal{A}} \pi(a|s)\:q_{\pi}(s,\:a)
\end{equation}

The other way around, by using the probabilities of the transition dynamics we can average through the values of the successing states we can evaluate a certain action:

\begin{equation}
q_{\pi}(s,\:a)\;=\;\mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{\pi}(s')
\label{eq:qn_avg}
\end{equation}

\begin{itemize}
\item $v$ is is telling how good it is to be in a particular state
\item $q$ is telling how good it is to take a particular action
\end{itemize}


Sticking these together in order to solve MDPs end up in a two-step lookahead:
\begin{itemize}
\item Consider all action we might take next $(v)$
\item Consider all the things the environment might do to us $(q)$
\item Evaluate the successor state after what the environment did after that point
\end{itemize}

By double-averaging over the policy as well as the transistion probability we get the answer to how good it is to be in a particular state (as the Bellman Equation):

\begin{equation}
v_{\pi}(s)\;=\;\sum_{a \in \mathcal{A}} \pi(a|s)\: \left( \: \mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{\pi}(s') \: \right)
\label{eq:v_pi_be}
\end{equation}

Starting from a particular action, we can also do the same two-step lookahead and see where the wind blows us, and from there consider which action might be taken next. By averaging the same way above we get same recursive relationship:

\begin{equation}
q_{\pi}(s,\:a)\;=\;\mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\: \sum_{a' \in \mathcal{A}} \pi(a'|s')\:q_{\pi}(s',\:a')
\end{equation}

This shows how the value- and action function of the next step relates to itself, we therefore get a recursive relationship. \textbf{After  all, the value function of the current time step is equal to the immediate reward plus the value function of where you end up.}


To flatten the MDP into a MRP one could average out the matrices as in (\ref{eq:averaging_mrp}) following policy $\pi$ and then solve accordingly as in (\ref{eq:mrp_matrix}). Thereby we can determine the value function.

.\newline
\textbf{\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\textbf{For more info on undiscounted MDPs:}

Neurodynamic Programming / Dynamic Programming;
Dimitri Bertsekas Dynamic Programming and Optimal Control (2 Vol Set)
\textbf{\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_} \newline

\subsection{The Optimal Value Function}
The optimal \textit{state-value} and \textit{action-value} functions are the maximum functions over all policies
\begin{equation}
v_{*}(s)\;=\;\mathop{max}_{\pi} \; v_{\pi}(s) \qquad q_{*}(s,a)\;=\;\mathop{max}_{\pi} \; q_{\pi}(s, a)
\end{equation}

\begin{itemize}
\item The optimal value function specifies the best performance in an MDP
\item An MDP is 'solved' when we know the optimal value function
\end{itemize}

If you know $q_{*}$, you're basically done. It tells you under all different ways to behave which one gives the most reward.

There is always one optimal policy. This optimal policy will then achieve the optimal \textit{state-value} and \textit{action-value} function $v_{\pi_{*}}$ and $q_{\pi_{*}}$. So we can define a partial ordering over policies:

\begin{equation}
\pi \: \geq \: \pi ' \quad if \quad v_{\pi}(s) \geq v_{\pi'}(s), \forall s
\end{equation}

An optimal policy can be found by maximising over $q_{\pi_{*}}(s,a)$:

\begin{equation}
\pi_{*}(a|s)\;=\; \Big\{ \begin{matrix} 
 1 \quad $if$ \;\; a \: = \: \mathop{argmax}_{a \in \mathcal{A}} \; q_{\pi_{*}}(s,a) \\
\; 0 \quad $otherweise$ \hphantom{abcdedgwfdwfdgh}
\end{matrix}
\end{equation}

\subsubsection*{The Bellman Optimality Equation}

The optimal value functions are recursively related by the Bellman optimality equations:

Instead of taking the average of all actions, you pick the maximum. That determines the optimal value function of a state:
\begin{equation}
v_{*} = \mathop{max}_{a} \; q_{*}(s, a)
\end{equation}

Now looking at the action-value function, we can inductively assume that each of the states we might end up in has a optimal state-value $v_{*}$. Therefore, similar as for (\ref{eq:qn_avg}), to determine the action-value we add the immediate reward to the average of the optimal state-values where we could end up in:

\begin{equation}
q_{*}(s,\:a)\;=\;\mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{*}(s')
\end{equation}

As we did before in (\ref{eq:v_pi_be}), we can put these two together to create the recursive relationship with the two-step lookahead. For the optimal solution, we pick the maximum instead of the average. This is the Bellman optimality equation for $v_{*}$:

\begin{equation}
v_{*}(s)\;=\;\mathop{max}_{a \in \mathcal{A}}\: \left( \: \mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{*}(s') \: \right)
\end{equation}

By reordering the same idea, we can again define the Bellman optimality equation for $q_{*}$. 

\begin{equation}
q_{*}(s,\:a)\;=\;\mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\: \sum_{a' \in \mathcal{A}} \mathop{max}_{a' \in \mathcal{A}} \; q_{*}(s', a')
\end{equation}

\subsubsection*{Solving the Bellman Optimality Equation}

\begin{itemize}
\item The Bellman Optimality Equation is non-linear \newline
$\qquad \rightarrow$ Solving by inverting matrices like in (\ref{eq:averaging_mrp}) will not work
\item No closed form solution (in general)
\item Many iterative solution methods \newline
$\qquad \rightarrow$ Value iteration, policy interation, \textbf{Q-Learning}, Sarsa
\end{itemize}
\newpage

\section{Lection Three: Planning by Dynamic Programming}
\subsection{Introduction}
\textbf{Dynamic}: sequential or temporal component to problem \newline
\textbf{Programming}: optimising a 'problem', e.g. a policy\newline

$\rightarrow$ Solving complex problems by braking them down into subproblems, then combine their solutions

\subsubsection*{Properties of problems for dynamic programming:}
\begin{itemize}
\item Optimal substructure: \\
$\rightarrow$ Principle of optimality applies \\
$\rightarrow$ Optimal solution can be decomposed into subproblems
\item Overlapping substructure \\
Subproblems reoccur many times $\rightarrow$ Solutions can be cached and reused
\end{itemize}

MDPs statisfy both properties. The Bellman Equation gives the recursive decompostion. The value function stores and reuses the solutions. Dynamic Programming thereby assumes full knowledge of the MDP, it is used for \textit{planning} in an MDP.

\subsubsection*{For prediction}
\begin{itemize}
\item \textbf{Input:} An MDP $\langle \mathcal{S,A,P,R,\gamma} \rangle$ and policy $\pi$ or an MRP $\langle \mathcal{S,P^{\pi},R^{\pi},\gamma} \rangle$ 
\item \textbf{Output:} value function $v_{\pi}$
\end{itemize}
or
\subsubsection*{For Optimal Control}
\begin{itemize}
\item \textbf{Input:} An MDP $\langle \mathcal{S,A,P,R,\gamma} \rangle$ 
\item \textbf{Output:} Optimal value function $v_{*}$ and optimal policy $\pi_{*}$
\end{itemize}

\subsection{Synchronous Dynamic programming algorithms}
\subsubsection*{Policy Evaluation and Policy Iteration}
Solution: iterative application of Bellman Equation $v_{1} \rightarrow v_{2}  \rightarrow v_{3}  \rightarrow \ldots \ v_{n}$
Using \textit{synchronous} backups:
\begin{itemize}
\item At each iteration k + 1
\item For all states $s \in \mathcal{S}$
\item Update $v_{k+1}(s)$ from $v_{k}(s')$ where s' is a successor state of s
\end{itemize}

Given a policy $\pi$, how do zou find out if it's optimal?
\begin{itemize}
\item \textit{Evaluate} the policy $\pi \rightarrow$ Compute the value function $v_{\pi}$
\item \textit{Improve} the policy by acting greedily with respect to $v_{\pi}$
\end{itemize}
Acting greedily means picking the action that returns the most reward. This will in every case improve the value function. In general, it always needs a lot of improvement and evaluation. This process will \textit{always} converge to $\pi_{*}$, no matter in which state you begin.

\includegraphics[scale=0.35]{pictures/eval_impr.png}

\subsubsection*{What does policy improvement mean?}
Consider a deterministic policy $a = \pi(s)$
\begin{itemize}
\item We can \textit{improve} the policy by acting greedily:  $\pi'(s) = \mathop{argmax}_{a \in \mathcal{A}} \; q_{\pi_{*}}(s,a)$
\item This will improve the value from any state s over one step: \\
$q_{\pi}(s, \pi'(s)) \;= \; \mathop{max}_{a \in \mathcal{A}} \; q_{\pi_{*}}(s,a) \; \geq \; q_{\pi_{*}}(s,a)\; + \; v_{\pi}$
\item Keep iterating over the coming time-steps
\item If improvement stops: \\
 $q_{\pi}(s, \pi'(s)) \;= \; \mathop{max}_{a \in \mathcal{A}} \; q_{\pi}(s,a) \; = \; q_{\pi}(s,a)\; = \; v_{\pi}$ \\
 $\qquad \rightarrow \qquad$ Bellman optimality equation is satisfied: $v_{\pi}(s)\;=\;\mathop{max}_{a \in \mathcal{A}} \; q_{\pi}(s,a)$
 \item Therefore: $v_{\pi} \; = \; v_{*}$ for all $s \in \mathcal{S} \qquad \rightarrow \qquad \pi$ is an optimal policy
\end{itemize}

\subsubsection*{Value iteration}
Any optimal policy can be subdivided into two components:
\begin{itemize}
\item An optimal first action $A_{*}$
\item Followed by an optimal policy from successor state $S'$
\end{itemize}
The policy is optimal, if from each state we might end up the policy is optimal from that state onwards.
If we know the solution to subproblems $v'_{*}(s)$, the solution $v_{*}(s)$ can be found iteratively by one-step lookahead:
\begin{equation}
v_{*}(s)\; \leftarrow \; \mathop{max}_{a \in \mathcal{A}}\: \left( \: \mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{*}(s') \: \right)
\end{equation}
The idea of value iteration is to apply these updates iteratively, thereby moving through the whole state-space. The intuition is that you begin with your final reward and work your way backwards.

Unlike policy iteration, there is no explicit policy. Intermediate value functions may not correspond to any policy.

\includegraphics[scale=0.2]{pictures/synchronous_algorithms.jpg}

These algorithms are based on the state-value function $v_{\pi}(s)$ or $v_{*}(s)$. They have a complexity of $\mathcal{O}(mn^2)$ per iteration, for $m$ actions and $n$ states. The algorithms could also apply to $q_{\pi}(s)$ or $q_{*}(s)$, where the complexity would be $\mathcal{O}(m^{2}n^{2})$.

\subsection{Asynchronous dynamic programming}
It is not necessary to update every state at each sweep of the algorithm. In \textit{asynchronous} backups, the relationship of updating every state in each iteration is broken. This saves a lot of unnecessary work and as long as all states keep being selected at some time, the algorithm will still converge to the optimal solution.

\subsubsection*{In-place dynamic programming}
Synchronous value iteration stores two copies of the value function:
\begin{align}
v_{new}(s)\; \leftarrow \; \mathop{max}_{a \in \mathcal{A}}\: \left( \: \mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{old}(s') \: \right) \\ 
v_{old} \; \leftarrow \; v_{new}
\end{align}
In-place value iteration only stores one copy of value function by using the latest version of the value function available. That means if a state has been visited before, the algorithm uses the already calculated value instead of the old one. This tends to be more efficient, but the order of the states being swept is not relevant for the efficiency of the algorithm.

\subsubsection*{Prioritized sweeping}
By using the magnitude of the Bellman error to guide state selection, the states with the largest difference after an update have a higher priority. These will have the most effect in downstream computing. It is important though to update the Bellman error after each backup, which required knowledge of reverse-dynamics of a system for the predecessor states. The Bellman error is described as following:

\begin{equation}
\Big| \;  \mathop{max}_{a \in \mathcal{A}}\: \left( \: \mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{old}(s') \: \right) \; - \; v(s) \; \Big|
\end{equation}

\subsubsection*{Real-time Dynamic Programming}
The idea of \textit{Real-time programming} is to only update the states which are relevant to the agent. The experience of the agent is used to guide the the selection of states. After each time-step $S_{t}, A_{t}, R_{t+1}$, a backup of state $S_{t}$ is done.

\begin{equation}
v(S_{t})\; \leftarrow \; \mathop{max}_{a \in \mathcal{A}}\: \left( \: \mathcal{R}_{S_{t}}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{S_{t}s'}^{a}\:v(s') \: \right) 
\end{equation}

\subsubsection*{Full-Width and Sample backups}
Dynamic programming uses \textit{full-width backups}, meaning that every state is backed up. For medium-sized problems consisting of a few millions of states, this is fine. But for large problems, dynamic programming suffer from the \textit{curse of dimensionality}, following from the exponential growth of the number of states with the number of state variables. 

This is where sample backups come into play. Instead of backing up the whole state-space (which can be already too expensive for one iteration), only certain samples are backed up. The advantages are:
\begin{itemize}
\item Model-free: No advance knowledge of MDP required
\item Breaks the curse of dimensionality through sampling
\item Cost of backup is constatnt, independant of $|n|=\mathcal{S}$ 
\end{itemize}
\newpage

\section{Lecture Four: Model-Free Prediction}
\subsection{Monte-Carlo Learning}
Monte-Carlo Learning methods learn directly from episodes of experience. It is therefore important that the MDP is applied to \textit{episodic} MDP, where the (complete) episodes do terminate. The value function is assumed to be the simplest possible, the mean return of the samples.

The goal is to learn the value function $v_{\pi}$ of a random policy $\pi$ by looking at some streams $S_{1}, A_{1}, R_{2}, \ldots, S_{k} \sim \pi$ of episode of experience and then evaluate the \textit{return} as the total discounted reward $\mathcal{G}_{t}$ form (\ref{eq:g_t}). Instead of using the \textit{expected} return of the value function, Monte-Carlo Learning uses the \textit{empirical mean} from the sampled episodes.

\subsubsection*{First-Visit Monte-Carlo policy evaluation}
\begin{itemize}
\item To evaluate the state $s$, the first time-step it is visited in an episode, an incremental counter is initialized: 
$N(s) \leftarrow N(s) + 1$
\item The total return is added up: $S(s) \leftarrow S(s) + G_{t}$
\item The value of the state is estimated by mean return: $V(s) = S(s) / N(s)$
\end{itemize}
By the law of large numbers, the value of the state $V(s)$ converges to the actual value $v_{\pi}$ of the state is visited many times so that the estimated mean consists of a large number of visits: $V(s) \rightarrow v_{\pi}$ as $N(s) \rightarrow \infty$

The \textbf{Every-Visit Monte-Carlo policy evaluation} works in a very similar fashion, just that the counter and return is incremented - as the name suggests - every time the episode visits a state.

\subsubsection*{Incremental Monte-Carlo}
The mean $\mu_{1}, \mu_{2}, \ldots$ of a sequence $x_{1}, x_{2}, \ldots$ can be computed incrementally:
\begin{equation}
\begin{aligned}
\mu_{k}\; &=\;\frac{1}{k}\:\sum_{j=1}^{k}\:x_{j}\\
&=\;\frac{1}{k}\:\big( x_{k} \:+\: \sum_{j=1}^{k-1}\:x_{j}\: \big) \\
&=\;\frac{1}{k}\:(\:x_{k} \: + \: (k-1)\:\mu_{k-1} \: )\\
&=\; \mu_{k-1} \: + \: \frac{1}{k}\:(\:x_{k} \: - \:\mu_{k-1} \: )
\end{aligned}
\label{eq:incr_mean}
\end{equation}
The difference $x_{k} - \mu_{k-1}$ basically defines the error term, where $\mu_{k-1}$ represents the expected value and $x_{k}$ the actual value. In the process of (\ref{eq:incr_mean}), the value of the mean will then get an update slightly into the direction of the error term.

This can be applied to the Monte-Carlo Learning. The value $V(s)$ of a state $s$ will be updated incrementally after an episode $S_{1}, A_{1}, R_{2}, \ldots, S_{t}$:
\begin{itemize}
\item For each state $S_{t}$ with return $G_{t}$: \\
$N(s) \leftarrow N(s) + 1$ \\
$V(S_{t}) \leftarrow V(S_{t}) + \frac{1}{N(s)} (G_{t} - V(S_{t}))$
\item In non-stationary problems, it can be useful to track a running mean, e.g. forget old episodes \\
$V(S_{t}) \leftarrow V(S_{t}) + \alpha (G_{t} - V(S_{t}))$
\end{itemize}

\subsection*{Temporal-Difference Learning}
Like with Monte-Carlo Learning, Temporal-Difference Learning learns from episodes of experience and needs no knowledge of the MDP transitions or rewards, they are both \textit{model-free}. In contrast to Monte Carlo Learning however, Temporal-Difference Learning also learns from incomplete episodes, they don't have to terminate. By taking partial episodes and estimating the remaining return, it can learn from incomplete episodes. It iteratively updates the guesses of the return while going thorugh an episode. This is called \textit{bootstrapping}.

\subsubsection*{The simplest Temporal-Difference Learning algorithm: TD(0)}
\begin{itemize}
\item The value of $V_{s}$ is updated towards the \textit{estimated} return: $R_{t+1}+\gamma V(S_{t+1})$
\begin{align*}
V(S_{t}) \leftarrow V(S_{t}) + \alpha (R_{t+1}+\gamma V(S_{t+1}) - V(S_{t}))
\end{align*}
\item $R_{t+1}+\gamma V(S_{t+1})$     is called the TD target (the estimated return)
\item $\delta_{t} = (R_{t+1}+\gamma V(S_{t+1}) - V(S_{t}))$     is called the TD error
\end{itemize}

\includegraphics[scale=0.3]{pictures/mc_td.jpg}
\caption{The difference between Monte-Carlo and Temporal-Difference Learning.}
\label{img:mc_td}
\end{document}

As seen in caption \ref{img:mc_td}, the predicted total travel time gets updated after every state in TD Learning, while in MC learning all the values of the states will be updated after the episode.