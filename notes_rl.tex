\documentclass[10pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{xparse}

\begin{document}
\section{Lecture two: Markov Decision Process (MDP)}
\subsection{Markov Process}

The current state characterises the process -> we are told the state --> environment is fully observable\newline

Almost all RL problems can be formalised as MDPs\newline
--> Optimal control primarily deals with continious MDPs\newline
--> Any partially observable problems can be converted into MDPs\newline
--> Bandits are MDPs with one state\newline

"The future is independent of the past given the present"

\begin{equation}
	\mathbb{P} \: [S_{t+1}\; | \; S_{t}] \; = \; \mathbb{P} \: [S_{t+1} \; | \; S_{1}, \ldots , S_{t}]
\end{equation}


What happens next only depends on what happend on the state before - you can throw away anything else.\newline

For a Markov state \textit{s} and successor state \textit{s'}, the \textit{state transition probability} is defined as

\begin{equation}
\mathcal{P}_{ss'} \; = \; \mathbb{P}\:[S_{t+1}\:=\:\textit{s'}\; | \; S_{t}\:=\:\textit{s}]
\end{equation}

State transition matrix $\rho$ defines transition probabilites from all states \textit{s} to all successor states \textit{s'}.

\begin{equation}
\mathcal{P} \; = \; from 
\stackrel{\mbox{$to$}}{\begin{pmatrix}
\mathcal{P}_{11} & \ldots & \mathcal{P}_{1n} \\
\vdots & & \\
\mathcal{P}_{n1} & \ldots & \mathcal{P}_{nn} \\
\end{pmatrix}}
\end{equation}

Each row of the matrix sums to 1! \newline
A Markov process is a memoryless random process, i.e. a sequence of random states, $S_{1}, \: S_{2}, \: \ldots$ with the Markov propoerty. It is defined as a tuple $\langle \mathcal{S}, \: \mathcal{P} \rangle$.
\begin{itemize}
\item S is a (finite) set of states
\item $\mathcal{P}$ is a state transition probability matrix
\end{itemize}

\subsection{Markov Reward process (MRP)}

The Markov Reward Process is defined as a tuple $\langle \mathcal{S}, \: \mathcal{P}, \: \mathcal{R}, \: \gamma \rangle$.
\begin{itemize}
\item $\mathcal{S}$ is a (finite) set of states
\item $\mathcal{P}$ is a state transition probability matrix
\item $\mathcal{R}$ is a reward function, $\mathcal{R}_{s} \: = \: \mathbb{E}[R_{t+1} \: | \: S_{t} = s]$
\item $\gamma$ is a discount factor, $\gamma \in [0,1]$
\end{itemize}

The \textit{return} $\mathcal{G}_{t}$ is the total discounted reward from time-step \textit{t}.

\begin{equation}
\mathcal{G}_{t} \; = \; R_{t+1} \; + \; \gamma R_{t+2} \; + \; \ldots \; = \;  \sum_{k=0}^{\infty} \;\gamma^{k} \: R_{t+k+1} 
\end{equation}

\begin{itemize}
\item The discount $\gamma$ is the present value of future rewards - the closer $\gamma$ is to zero, the less are later rewards accounted (e.g. more 'short-sighted').
\item The value of receiving reward $\mathcal{R}$ after $k+1$ time-steps is $ \gamma^{k} R$.
\end{itemize}

\subsubsection*{Why discount?}
\begin{itemize}
\item Unless you really trust your model and believe that everything turns out as planned, you need to discount in deviations - Uncertainty about the future may not be fully represented
\item Mathematically convenient to discount rewards, avoids infinite returns
\item If the reward is financial, immediate rewards may earn more interest than delayed rewards
\item Animal/human behaviour shows preference for immediate reward
\item It is sometimes possible to use \textit{undiscounted} Markov reward process (i.e. $\gamma = 1$), e.g. if all sequences terminate.
\end{itemize}

\subsubsection*{The value function}

The value function \textit{v(s)} gives the long-term value of state s. It defines the expected return in a MRP starting from state \textit{s}:

\begin{equation}
v(s) \; = \; \mathbb{E}\;[G_{t} \; | \; S_{t} \; = \; s]
\end{equation}

The value function can be decomposed into two parts:

\begin{itemize}
\item immediate reward $R+1$
\item discounted value of successor state $\gamma v(S_{t+1})$
\end{itemize}

This resolves into the \textbf{Bellman equation for MRPs}:
\begin{equation}
v(s) \; = \; \mathbb{E}\;[G_{t} \; | \; S_{t} \; = \; s] \; = \; \mathbb{E}\;[R_{t+1} \; + \; \gamma v(S_{t+1}) \; | \; S_{t} \; = \; s]
\end{equation}

By averaging all possible outcomes we get
\begin{equation}
v(s) \; = \; \mathcal{R}_{s} \; + \; \gamma \: \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}\:v(s')
 \label{eq:averaging_mrp}
\end{equation}

The Bellman  equation can be concisely using matrices, where $v$ is a column vector with one entry per state

\begin{equation}
v \; = \; \mathcal{R} \; + \; \gamma \: \mathcal{P} \: v \quad \rightarrow \quad
\begin{pmatrix}
v(1) \\ \vdots \\ v(n) \end{pmatrix} \; = \; \begin{pmatrix}
\mathcal{R}_{1} \\ \vdots \\ \mathcal{R}_{n} \end{pmatrix} \; + \; \gamma \; \begin{pmatrix}
\mathcal{P}_{11} & \ldots & \mathcal{P}_{1n} \\
\vdots & & \\
\mathcal{P}_{n1} & \ldots & \mathcal{P}_{nn} \\ \end{pmatrix} \; \begin{pmatrix}
v(1) \\ \vdots \\ v(n) \end{pmatrix} \label{eq:mrp_matrix}
\end{equation} 

The Bellman equation is linear. It can be solved directly by $v = (I - \gamma \mathcal{P})^{-1} \mathcal{R}$.

\begin{itemize}
\item The Computational complexity is O($n^{3}$) for \textit{n} states.
\item Direct solution only possible for small MRPs
\item Iterative methods for large MRPs, e.g. Dynamic programming, Monte-Carlo evaluation, Temporal-Difference learning
\end{itemize}

\subsection{Markov Decision Process (MDP)}
A MDP is a MRP with decisions. It is an \textit{environment} in which all states are Markov. \newline
A MDP is a tuple $\langle \mathcal{S, A, P, R,} \gamma \rangle$.

\begin{itemize}
\item $\mathcal{S}$ is a (finite) set of states
\item $\mathcal{A}$ is a (finite) set of actions
\item $\mathcal{P}$ is a state transition probability matrix \newline
$\mathcal{P}_{ss'}^{a} \; = \; \mathbb{P}\:[S_{t+1}\:=\:\textit{s'}\; | \; S_{t}\:=\:\textit{s}, \; A_{t}\:=\:\textit{a}]$
\item $\mathcal{R}$ is a reward function, $\mathcal{R}_{s}^{a}  \: = \: \mathbb{E}[R_{t+1} \: | \: S_{t} = s, \; A_{t}\:=\:\textit{a}]$
\item $\gamma$ is a discount factor, $\gamma \in [0,1]$
\end{itemize}

A policy $\pi$ is a distribution over actions given states. It fully defines the behaviour of an agent.
\begin{equation}
\pi(a|s) \; = \; \mathbb{P}\:[\:\: S_{t} = s, \; A_{t}\:=\:\textit{a}\:]
\end{equation}

In an MDP, the policies depend on the current state (not the history). Policies are stationary: $A_{t} = \pi ( \dot | S_{t} ), \forall t > 0$ \newline

Given an MDP $\mathcal{M} = \langle \mathcal{S, A, P, R,} \gamma \rangle$ and a policy $\pi$:
\begin{itemize}
\item The state sequence $S_{1}, S_{2}, \ldots$ is a Markov process $\langle \mathcal{S, P^{\pi}} \rangle$
\item The state and reward sequence $S_{1}, ,R_{2}, S_{2}, \ldots$ is a Markov reward process $\langle \mathcal{S}, \: \mathcal{P}, \: \mathcal{R}, \: \gamma \rangle$ where
\begin{equation}
\mathcal{P}_{s,s'}^{\pi} \; = \; \sum_{a \in \mathcal{A}} \; \pi (a|s)\: \mathcal{P}_{s,s'}^{a} \qquad
\mathcal{R}_{s}^{\pi} \; = \; \sum_{a \in \mathcal{A}} \; \pi (a|s)\: \mathcal{R}_{s}^{a} 
\end{equation}
\end{itemize}

The \textit{state-value function} $v_{\pi}(s)$ of an MDP is the expected return starting from state \textit{s}, and then following policy $\pi$:
\begin{equation}
v_{\pi}(s) \; = \; \mathbb{E}_{\pi} [ G_{t} \: | \: S_{t} \: = \: s] 
\end{equation}

The \textit{action-value function} $q_{\pi}(s, a)$ of an MDP is the expected return starting from state \textit{s}, taking action \textit{a}, and then following policy $\pi$:
\begin{equation}
q_{\pi}(s,a) \; = \; \mathbb{E}_{\pi} [ G_{t} \: | \: S_{t} \: = \: s, \: A_{t} \: = \: a] 
\end{equation}

The state-value and action-value functions can again be decomposed into Bellman equations consisting of immediate reward plus discounted value of successor state:

\begin{equation}
v_{\pi} \; = \; \mathbb{E}_{\pi} \:[ R_{t+1} \: + \: \gamma v_{\pi} (S_{t+1}) \: \ | \: S_{t} \: = \: s]
\end{equation}
\begin{equation}
q_{\pi} \; = \; \mathbb{E}_{\pi} \:[ R_{t+1} \: + \: \gamma v_{\pi} (S_{t+1}, \: A_{t+1}) \: \ | \: S_{t} \: = \: s, \: A_{t} \: = \: a]
\end{equation}

Basically, the state-value averages over the different actions that can be taken:

\begin{equation}
v_{\pi}(s) \; = \; \sum_{a \in \mathcal{A}} \pi(a|s)\:q_{\pi}(s,\:a)
\end{equation}

The other way around, by using the probabilities of the transition dynamics we can average through the values of the successing states we can evaluate a certain action:

\begin{equation}
q_{\pi}(s,\:a)\;=\;\mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{\pi}(s')
\label{eq:qn_avg}
\end{equation}

\begin{itemize}
\item $v$ is is telling how good it is to be in a particular state
\item $q$ is telling how good it is to take a particular action
\end{itemize}


Sticking these together in order to solve MDPs end up in a two-step lookahead:
\begin{itemize}
\item Consider all action we might take next $(v)$
\item Consider all the things the environment might do to us $(q)$
\item Evaluate the successor state after what the environment did after that point
\end{itemize}

By double-averaging over the policy as well as the transistion probability we get the answer to how good it is to be in a particular state (as the Bellman Equation):

\begin{equation}
v_{\pi}(s)\;=\;\sum_{a \in \mathcal{A}} \pi(a|s)\: \left( \: \mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{\pi}(s') \: \right)
\label{eq:v_pi_be}
\end{equation}

Starting from a particular action, we can also do the same two-step lookahead and see where the wind blows us, and from there consider which action might be taken next. By averaging the same way above we get same recursive relationship:

\begin{equation}
q_{\pi}(s,\:a)\;=\;\mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\: \sum_{a' \in \mathcal{A}} \pi(a'|s')\:q_{\pi}(s',\:a')
\end{equation}

This shows how the value- and action function of the next step relates to itself, we therefore get a recursive relationship. \textbf{After  all, the value function of the current time step is equal to the immediate reward plus the value function of where you end up.}


To flatten the MDP into a MRP one could average out the matrices as in (\ref{eq:averaging_mrp}) following policy $\pi$ and then solve accordingly as in (\ref{eq:mrp_matrix}). Thereby we can determine the value function.

.\newline
\textbf{\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\textbf{For more info on undiscounted MDPs:}

Neurodynamic Programming / Dynamic Programming;
Dimitri Bertsekas Dynamic Programming and Optimal Control (2 Vol Set)
\textbf{\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_} \newline

\subsection{The Optimal Value Function}
The optimal \textit{state-value} and \textit{action-value} functions are the maximum functions over all policies
\begin{equation}
v_{*}(s)\;=\;\mathop{max}_{\pi} \; v_{\pi}(s) \qquad q_{*}(s,a)\;=\;\mathop{max}_{\pi} \; q_{\pi}(s, a)
\end{equation}

\begin{itemize}
\item The optimal value function specifies the best performance in an MDP
\item An MDP is 'solved' when we know the optimal value function
\end{itemize}

If you know $q_{*}$, you're basically done. It tells you under all different ways to behave which one gives the most reward.

There is always one optimal policy. This optimal policy will then achieve the optimal \textit{state-value} and \textit{action-value} function $v_{\pi_{*}}$ and $q_{\pi_{*}}$. So we can define a partial ordering over policies:

\begin{equation}
\pi \: \geq \: \pi ' \quad if \quad v_{\pi}(s) \geq v_{\pi'}(s), \forall s
\end{equation}

An optimal policy can be found by maximising over $q_{\pi_{*}}(s,a)$:

\begin{equation}
\pi_{*}(a|s)\;=\; \Big\{ \begin{matrix} 
 1 \quad $if$ \;\; a \: = \: \mathop{argmax}_{a \in \mathcal{A}} \; q_{\pi_{*}}(s,a) \\
\; 0 \quad $otherweise$ \hphantom{abcdedgwfdwfdgh}
\end{matrix}
\end{equation}

\subsubsection*{The Bellman Optimality Equation}

The optimal value functions are recursively related by the Bellman optimality equations:

Instead of taking the average of all actions, you pick the maximum. That determines the optimal value function of a state:
\begin{equation}
v_{*} = \mathop{max}_{a} \; q_{*}(s, a)
\end{equation}

Now looking at the action-value function, we can inductively assume that each of the states we might end up in has a optimal state-value $v_{*}$. Therefore, similar as for (\ref{eq:qn_avg}), to determine the action-value we add the immediate reward to the average of the optimal state-values where we could end up in:

\begin{equation}
q_{*}(s,\:a)\;=\;\mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{*}(s')
\end{equation}

As we did before in (\ref{eq:v_pi_be}), we can put these two together to create the recursive relationship with the two-step lookahead. For the optimal solution, we pick the maximum instead of the average. This is the Bellman optimality equation for $v_{*}$:

\begin{equation}
v_{*}(s)\;=\;\mathop{max}_{a}\: \left( \: \mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\:v_{*}(s') \: \right)
\end{equation}

By reordering the same idea, we can again define the Bellman optimality equation for $q_{*}$. 

\begin{equation}
q_{*}(s,\:a)\;=\;\mathcal{R}_{s}^{a}\;+\;\gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^{a}\: \sum_{a' \in \mathcal{A}} \mathop{max}_{a'} \; q_{*}(s', a')
\end{equation}

\subsubsection*{Solving the Bellman Optimality Equation}

\begin{itemize}
\item The Bellman Optimality Equation is non-linear \newline
$\qquad \rightarrow$ Solving by inverting matrices like in (\ref{eq:averaging_mrp}) will not work
\item No closed form solution (in general)
\item Many iterative solution methods \newline
$\qquad \rightarrow$ Value iteration, policy interation, \textbf{Q-Learning}, Sarsa
\end{itemize}
\newline 

\end{document}